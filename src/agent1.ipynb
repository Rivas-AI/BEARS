{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9218aed51921c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:28:55.089969Z",
     "start_time": "2025-05-09T18:28:55.086426Z"
    }
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "import requests\n",
    "import itertools\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "AGENT_1 = 'llama3.2:3b'\n",
    "BEARER_TOKEN = \"insert your semantic scholar key here\"\n",
    "SS_HEADERS = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}\n",
    "KEYWORD_PERMUTATION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810be9687248462b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:02.269995Z",
     "start_time": "2025-05-09T18:28:55.118729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting keyword generation process...\n",
      "Sending prompt to Ollama model: llama3.2:3b...\n",
      "Received response from Ollama.\n",
      "Initial API call generated 37 unique keywords\n",
      "Attempt 1: Need 13 more keywords\n",
      "Sending prompt to Ollama model: llama3.2:3b...\n",
      "Received response from Ollama.\n",
      "Added 14 new unique keywords\n",
      "\n",
      "--- Final List of Keywords ---\n",
      "Total unique keywords: 50\n",
      "\n",
      "Keywords as a numbered list:\n",
      "1. climate modeling\n",
      "2. climate data\n",
      "3. Arctic anomaly detection\n",
      "4. deep learning\n",
      "5. computer vision\n",
      "6. climate prediction\n",
      "7. machine learning\n",
      "8. predictive analytics\n",
      "9. climate change mitigation\n",
      "10. renewable energy systems\n",
      "11. sustainable development\n",
      "12. green technology\n",
      "13. artificial intelligence\n",
      "14. climate modeling uncertainties\n",
      "15. climate resilience\n",
      "16. geoengineering\n",
      "17. climate tipping points\n",
      "18. weather forecasting\n",
      "19. satellite imagery\n",
      "20. ocean currents\n",
      "21. sea level rise\n",
      "22. climate adaptation strategies\n",
      "23. carbon footprint analysis\n",
      "24. deep learning for climate modeling\n",
      "25. climate change impacts\n",
      "26. climate modeling error analysis\n",
      "27. machine learning for climate prediction\n",
      "28. climate data visualization\n",
      "29. climate informatics\n",
      "30. climate change policy\n",
      "31. sustainability science\n",
      "32. environmental modeling\n",
      "33. climate governance\n",
      "34. climate justice\n",
      "35. climate change ethics\n",
      "36. climate decision-making\n",
      "37. deep learning algorithms\n",
      "38. deep learning for climate resilience\n",
      "39. climate modeling interpretability\n",
      "40. machine learning based weather forecasting\n",
      "41. climate data fusion\n",
      "42. artificial intelligence in climate governance\n",
      "43. predictive analytics for climate tipping points\n",
      "44. climate change scenario planning\n",
      "45. deep learning algorithms for sea level rise prediction\n",
      "46. sustainable deep learning models\n",
      "47. climate informatics for carbon footprint analysis\n",
      "48. machine learning for climate adaptation strategies\n",
      "49. climate decision support systems\n",
      "50. environmental predictive modeling\n",
      "Keywords successfully saved to output/keywords.json\n"
     ]
    }
   ],
   "source": [
    "def query_ollama_model(model_name, prompt_text):\n",
    "    try:\n",
    "        print(f\"Sending prompt to Ollama model: {model_name}...\")\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt_text,\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        print(\"Received response from Ollama.\")\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while communicating with Ollama: {e}\"\n",
    "\n",
    "def get_full_keyword_list(model_name, initial_prompt, target_count=50):\n",
    "    # Get initial keywords\n",
    "    keywords_response = query_ollama_model(model_name, initial_prompt)\n",
    "\n",
    "    if \"An error occurred\" in keywords_response:\n",
    "        print(\"Error in initial keyword generation\")\n",
    "        return []\n",
    "\n",
    "    # Process the keywords\n",
    "    keywords = [kw.strip() for kw in keywords_response.split(',') if kw.strip()]\n",
    "    unique_keywords = list(dict.fromkeys(keywords))  # Remove duplicates while preserving order\n",
    "\n",
    "    print(f\"Initial API call generated {len(unique_keywords)} unique keywords\")\n",
    "\n",
    "    # Continue requesting more keywords until we reach the target\n",
    "    attempt = 1\n",
    "    max_attempts = 5\n",
    "\n",
    "    while len(unique_keywords) < target_count and attempt < max_attempts:\n",
    "        current_count = len(unique_keywords)\n",
    "        remaining = target_count - current_count\n",
    "\n",
    "        print(f\"Attempt {attempt}: Need {remaining} more keywords\")\n",
    "\n",
    "        # Create a reprompt that includes existing keywords\n",
    "        reprompt = f\"\"\"Based on the following existing keywords about Climate Change and Deep Learning:\n",
    "\n",
    "        {', '.join(unique_keywords)}\n",
    "\n",
    "        Please generate {remaining} ADDITIONAL unique concise keywords/terms that are NOT in the above list. These should be at the intersection of Climate Change and Deep Learning.\n",
    "\n",
    "        Output ONLY the new keywords as a comma-separated list with no additional text, numbering, or formatting.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get additional keywords\n",
    "        additional_response = query_ollama_model(model_name, reprompt)\n",
    "\n",
    "        if \"An error occurred\" not in additional_response:\n",
    "            # Process and add new keywords\n",
    "            new_keywords = [kw.strip() for kw in additional_response.split(',') if kw.strip()]\n",
    "\n",
    "            # Add only unique new keywords\n",
    "            for kw in new_keywords:\n",
    "                if kw not in unique_keywords:\n",
    "                    unique_keywords.append(kw)\n",
    "\n",
    "            print(f\"Added {len(unique_keywords) - current_count} new unique keywords\")\n",
    "        else:\n",
    "            print(\"Error in additional keyword generation\")\n",
    "\n",
    "        attempt += 1\n",
    "\n",
    "    # Trim to exact target count if we got more than needed\n",
    "    if len(unique_keywords) > target_count:\n",
    "        unique_keywords = unique_keywords[:target_count]\n",
    "\n",
    "    return unique_keywords\n",
    "\n",
    "def save_keywords_to_json(keywords, output_path=\"output/keywords.json\"):\n",
    "    \"\"\"\n",
    "    Saves the list of keywords to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): List of keywords to save\n",
    "        output_path (str): Path where the JSON file will be saved\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Create a dictionary with the keywords\n",
    "        keyword_data = {\n",
    "            \"count\": len(keywords),\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "\n",
    "        # Save to JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(keyword_data, f, indent=2)\n",
    "\n",
    "        print(f\"Keywords successfully saved to {output_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving keywords to JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # The prompt for Agent 1: Paper Retriever to generate keywords\n",
    "    # for the topic \"Climate Change and Deep Learning\"\n",
    "    PROMPT_FOR_KEYWORDS = f\"\"\"As an expert keyword generator for academic research, your task is to produce a comprehensive list of keywords. These keywords are for querying the Semantic Scholar API to find papers at the intersection of <Topic>Climate Change and Deep Learning</Topic>. The goal is to uncover original and novel research directions.\n",
    "\n",
    "    Instructions for keyword generation:\n",
    "    1.  Focus on combining concepts from 'Climate Change' and 'Deep Learning'.\n",
    "    2.  Aim for keywords that are specific and could lead to novel research areas or innovative applications.\n",
    "    3.  The output MUST be a single string containing the keywords.\n",
    "    4.  Each keyword or keyword phrase MUST be separated by a comma and a space. For example: 'keyword1, keyword2, keyword3'.\n",
    "    5.  Do NOT use bullet points, numbered lists, introductory phrases (like \"Here is a list...\"), or any other formatting. Only provide the comma-separated list of keywords.\n",
    "\n",
    "    Generate a diverse set of 50 of these keywords. The generated keywords should be strictly 50 and only 50. Can't be more or less. Example of desired output format: 'Climate modeling, climate data, Arctic anomaly detection, Deep Learning, Computer Vision,Climate Prediction'.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Execution ---\n",
    "    print(\"Starting keyword generation process...\")\n",
    "\n",
    "    # Get a full list of 50 keywords with reprompting if needed\n",
    "    complete_keywords = get_full_keyword_list(AGENT_1, PROMPT_FOR_KEYWORDS, 50)\n",
    "\n",
    "    print(\"\\n--- Final List of Keywords ---\")\n",
    "    print(f\"Total unique keywords: {len(complete_keywords)}\")\n",
    "\n",
    "    if complete_keywords:\n",
    "        print(\"\\nKeywords as a numbered list:\")\n",
    "        for i, kw in enumerate(complete_keywords):\n",
    "            print(f\"{i+1}. {kw}\")\n",
    "\n",
    "        save_keywords_to_json(complete_keywords)\n",
    "    else:\n",
    "        print(\"Failed to generate keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c591f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:13.687147Z",
     "start_time": "2025-05-09T18:29:02.289263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8 permutations\n",
      "Processing query 1/8: climate modeling OR climate data OR Arctic anomaly detection OR deep learning\n",
      "Search error for query 'climate modeling OR climate data OR Arctic anomaly detection OR deep learning': 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=climate+modeling+OR+climate+data+OR+Arctic+anomaly+detection+OR+deep+learning&limit=15&fields=paperId%2Ctitle%2CexternalIds%2Cabstract\n",
      "Found 0 papers, 0 unique papers so far\n",
      "Processing query 2/8: climate modeling OR climate data OR Arctic anomaly detection OR computer vision\n",
      "Found 0 papers, 0 unique papers so far\n",
      "Processing query 3/8: climate modeling OR climate data OR Arctic anomaly detection OR climate prediction\n",
      "Search error for query 'climate modeling OR climate data OR Arctic anomaly detection OR climate prediction': 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=climate+modeling+OR+climate+data+OR+Arctic+anomaly+detection+OR+climate+prediction&limit=15&fields=paperId%2Ctitle%2CexternalIds%2Cabstract\n",
      "Found 0 papers, 0 unique papers so far\n",
      "Processing query 4/8: climate modeling OR climate data OR Arctic anomaly detection OR machine learning\n",
      "Search error for query 'climate modeling OR climate data OR Arctic anomaly detection OR machine learning': 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=climate+modeling+OR+climate+data+OR+Arctic+anomaly+detection+OR+machine+learning&limit=15&fields=paperId%2Ctitle%2CexternalIds%2Cabstract\n",
      "Found 0 papers, 0 unique papers so far\n",
      "Processing query 5/8: climate modeling OR climate data OR Arctic anomaly detection OR predictive analytics\n",
      "Found 1 papers, 1 unique papers so far\n",
      "Processing query 6/8: climate modeling OR climate data OR Arctic anomaly detection OR climate change mitigation\n",
      "Search error for query 'climate modeling OR climate data OR Arctic anomaly detection OR climate change mitigation': 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=climate+modeling+OR+climate+data+OR+Arctic+anomaly+detection+OR+climate+change+mitigation&limit=15&fields=paperId%2Ctitle%2CexternalIds%2Cabstract\n",
      "Found 0 papers, 1 unique papers so far\n",
      "Processing query 7/8: climate modeling OR climate data OR Arctic anomaly detection OR renewable energy systems\n",
      "Found 0 papers, 1 unique papers so far\n",
      "Processing query 8/8: climate modeling OR climate data OR Arctic anomaly detection OR sustainable development\n",
      "Found 1 papers, 1 unique papers so far\n",
      "\n",
      "================================================================================\n",
      "RESULTS: Found 1 unique papers across 8 queries\n",
      "================================================================================\n",
      "\n",
      "Paper 1/1\n",
      "ID:      ac54788018992f2216afc87e3f344cf0f4d2b5fd\n",
      "Title:   Quantifying the Influence of Climate on Storm Activity Using Machine Learning\n",
      "Abstract: Extratropical storms shape midlatitude weather and vary due to both the slowly evolving climate and the rapid changes in synoptic conditions. While the influence of each factor has been studied extensively, their relative importance remains unclear. Here, we quantify the climate's relative importance in both mean storm activity and individual storm development using 84 years of ERA-5 data and Convolutional Neural Networks (CNN). We find that the constructed CNN model predicts more than 90% of the variability in the mean storm activity. However, a similar model predicts up to a third of the variability in individual storm features, such as intensity, growth time, and storm trajectory, showing their variability is dominated by synoptic conditions. Isolating present-day climate change yields a signal-to-noise ratio (SNR) of about 0.16% for storm-intensity attribution, whereas the SNR for heat anomalies is ~50 times higher, highlighting that focusing on variables more directly tied to warming provides a clearer attribution pathway.\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "Most productive queries:\n",
      "- 'climate modeling OR climate data OR Arctic anomaly detection OR predictive analytics': 1 papers\n",
      "- 'climate modeling OR climate data OR Arctic anomaly detection OR sustainable development': 1 papers\n",
      "- 'climate modeling OR climate data OR Arctic anomaly detection OR deep learning': 0 papers\n",
      "- 'climate modeling OR climate data OR Arctic anomaly detection OR computer vision': 0 papers\n",
      "- 'climate modeling OR climate data OR Arctic anomaly detection OR climate prediction': 0 papers\n"
     ]
    }
   ],
   "source": [
    "# Function to generate keyword permutations\n",
    "def generate_keyword_permutations(keywords: List[str], set_size: int = KEYWORD_PERMUTATION) -> List[str]:\n",
    "    \"\"\"Generate all permutations of keywords in sets of set_size, joined with OR.\"\"\"\n",
    "    if len(keywords) < set_size:\n",
    "        print(f\"Warning: Only {len(keywords)} keywords available, but requested sets of {set_size}\")\n",
    "        set_size = len(keywords)\n",
    "\n",
    "    permutations = []\n",
    "    # Use combinations rather than permutations since order doesn't matter for OR searches\n",
    "    for combo in itertools.islice(itertools.combinations(keywords, set_size),8):\n",
    "        query = \" OR \".join(combo)\n",
    "        permutations.append(query)\n",
    "\n",
    "    print(f\"Generated {len(permutations)} permutations\")\n",
    "    return permutations\n",
    "\n",
    "\n",
    "# ——— 3) Function to search for papers with a given query ———\n",
    "def search_papers(query: str, limit: int = 15) -> List[Dict]:\n",
    "    \"\"\"Search for papers using the given query.\"\"\"\n",
    "    SEARCH_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "    search_params = {\n",
    "        \"query\": query,\n",
    "        \"limit\": limit,\n",
    "        \"fields\": \"paperId,title,externalIds,abstract\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(SEARCH_URL, params=search_params, headers=SS_HEADERS)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json().get(\"data\", [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Search error for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ——— 4) Function to get paper abstract from different sources ———\n",
    "def get_paper_abstract(paper: Dict) -> str:\n",
    "    \"\"\"Attempt to get abstract from Semantic Scholar, then OpenAlex if needed.\"\"\"\n",
    "    # Check if we already have the abstract\n",
    "    abstract = paper.get(\"abstract\")\n",
    "    if abstract:\n",
    "        return abstract\n",
    "\n",
    "    # If not, try to fetch it from Semantic Scholar\n",
    "    DETAIL_URL = \"https://api.semanticscholar.org/graph/v1/paper/\"\n",
    "    pid = paper[\"paperId\"]\n",
    "\n",
    "    try:\n",
    "        detail_params = {\"fields\": \"abstract,externalIds\"}\n",
    "        r = requests.get(DETAIL_URL + pid, params=detail_params, headers=SS_HEADERS)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        abstract = data.get(\"abstract\")\n",
    "\n",
    "        # If SS returned no abstract, try OpenAlex\n",
    "        if not abstract:\n",
    "            ext = data.get(\"externalIds\") or paper.get(\"externalIds\", {})\n",
    "            oa_id = ext.get(\"OpenAlex\") if ext else None\n",
    "\n",
    "            if oa_id:\n",
    "                oa_url = f\"https://api.openalex.org/works/{oa_id}\"\n",
    "                oa_r = requests.get(oa_url)\n",
    "                if oa_r.ok:\n",
    "                    oa_data = oa_r.json()\n",
    "                    inv_idx = oa_data.get(\"abstract_inverted_index\") or {}\n",
    "                    if inv_idx:\n",
    "                        # Reconstruct plain text abstract from inverted index\n",
    "                        pos_map = {}\n",
    "                        for word, positions in inv_idx.items():\n",
    "                            for pos in positions:\n",
    "                                pos_map[pos] = word\n",
    "                        # Build ordered list of words\n",
    "                        abstract = \" \".join(\n",
    "                            pos_map[i] for i in range(len(pos_map))\n",
    "                            if i in pos_map\n",
    "                        )\n",
    "\n",
    "        return abstract or \"(no abstract available)\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching abstract for paper {pid}: {e}\")\n",
    "        return \"(error retrieving abstract)\"\n",
    "\n",
    "\n",
    "# ——— 5) Main function to run the entire process ———\n",
    "def main():\n",
    "    # Generate permutations\n",
    "    query_permutations = generate_keyword_permutations(complete_keywords, set_size=4)\n",
    "\n",
    "    # Track unique papers by ID to avoid duplicates\n",
    "    unique_papers: Dict[str, Dict] = {}\n",
    "    papers_per_query: Dict[str, Set[str]] = {}\n",
    "\n",
    "    # Process each permutation\n",
    "    for i, query in enumerate(query_permutations):\n",
    "        print(f\"Processing query {i + 1}/{len(query_permutations)}: {query}\")\n",
    "\n",
    "        # Search for papers\n",
    "        papers = search_papers(query)\n",
    "        papers_per_query[query] = set()\n",
    "\n",
    "        for paper in papers:\n",
    "            pid = paper[\"paperId\"]\n",
    "            papers_per_query[query].add(pid)\n",
    "\n",
    "            # Skip if we already have this paper\n",
    "            if pid in unique_papers:\n",
    "                continue\n",
    "\n",
    "            # Get the abstract if not already included\n",
    "            if not paper.get(\"abstract\"):\n",
    "                paper[\"abstract\"] = get_paper_abstract(paper)\n",
    "\n",
    "            # Add to our collection of unique papers\n",
    "            unique_papers[pid] = paper\n",
    "        \n",
    "        if len(unique_papers) == 15:\n",
    "           break\n",
    "        print(f\"Found {len(papers)} papers, {len(unique_papers)} unique papers so far\")\n",
    "\n",
    "        # Respect rate limits (wait 1 second between queries)\n",
    "        time.sleep(1)\n",
    "    #save unique_papers as a csv file\n",
    "    df = pd.DataFrame(unique_papers.values())\n",
    "    df.to_csv(\"output/unique_papers.csv\", index=False)\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"RESULTS: Found {len(unique_papers)} unique papers across {len(query_permutations)} queries\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    with open(\"output/unique_paper.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(unique_papers, f, indent=2)\n",
    "\n",
    "    # Print each unique paper\n",
    "    for i, (pid, paper) in enumerate(unique_papers.items()):\n",
    "        title = paper.get(\"title\", \"(no title)\")\n",
    "        abstract = paper.get(\"abstract\", \"(no abstract available)\")\n",
    "\n",
    "        print(f\"Paper {i + 1}/{len(unique_papers)}\")\n",
    "        print(f\"ID:      {pid}\")\n",
    "        print(f\"Title:   {title}\")\n",
    "        print(f\"Abstract: {abstract}\\n\")\n",
    "        print(\"─\" * 80 + \"\\n\")\n",
    "\n",
    "    # Analyze which queries were most productive\n",
    "    query_productivity = [(query, len(pids)) for query, pids in papers_per_query.items()]\n",
    "    query_productivity.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nMost productive queries:\")\n",
    "    for query, count in query_productivity[:5]:\n",
    "        print(f\"- '{query}': {count} papers\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32e2013693fc30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:13.718155Z",
     "start_time": "2025-05-09T18:29:13.716685Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
