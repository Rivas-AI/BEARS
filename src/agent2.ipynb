{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:47.481872Z",
     "start_time": "2025-05-09T18:29:47.479250Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ollama\n",
    "\n",
    "# Configuration\n",
    "UNIQUE_PAPER = 'output/unique_paper.json'\n",
    "KEYWORDS = 'output/keywords.json'\n",
    "\n",
    "OUTPUT_FILE = f'output/agent2_ideas.json'\n",
    "\n",
    "AGENT_2 = 'qwen3:32b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:47.534724Z",
     "start_time": "2025-05-09T18:29:47.532600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_keywords(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:47.584968Z",
     "start_time": "2025-05-09T18:29:47.581335Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_ideas(keywords: list, paper: dict, model: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are Agent 2 (Idea Generator) in a climate-change deep learning pipeline.\n",
    "\n",
    "    Given the following keywords:\n",
    "    {keywords}\n",
    "\n",
    "    Given the following papers:\n",
    "    {paper}\n",
    "\n",
    "    Propose 15-20 candidate deep learning project ideas to combat climate change.\n",
    "\n",
    "    RESPONSE FORMAT:\n",
    "    Return your response as a simple formatted list with TITLE and DESCRIPTION for each idea, separated by blank lines. For example:\n",
    "\n",
    "    TITLE: Climate Tipping Point Prediction System\n",
    "    DESCRIPTION: A deep learning system that identifies potential climate tipping points by analyzing historical climate data and current trends. Uses recurrent neural networks to process time series data from multiple sources to predict non-linear climate transitions.\n",
    "    DATA NEEDS: Historical climate records, current sensor data, satellite imagery\n",
    "\n",
    "    TITLE: Carbon Sequestration Optimization Network\n",
    "    DESCRIPTION: An ML algorithm that determines optimal locations and methods for carbon sequestration based on geographical and atmospheric conditions. Combines computer vision analysis of terrain with climate models to maximize carbon capture efficiency.\n",
    "    DATA NEEDS: Historical climate records, current sensor data, satellite imagery\n",
    "\n",
    "    Do not include any additional text, explanations, introductions, or conclusions. Start directly with the first TITLE and end with the last DESCRIPTION. Ensure each project idea is separated by exactly one blank line from the next idea.\n",
    "\n",
    "    Now, generate 15-20 creative and technically feasible project ideas based on the provided keywords and papers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(model=model, messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama model: {e}\")\n",
    "        try:\n",
    "            response = ollama.generate(model=model, prompt=prompt)\n",
    "            return response['response']\n",
    "        except Exception as e2:\n",
    "            print(f\"Error in fallback generate: {e2}\")\n",
    "            return str(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:29:47.636162Z",
     "start_time": "2025-05-09T18:29:47.630176Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_ideas(output_str: str) -> list:\n",
    "    # Clean up fenced code blocks\n",
    "    cleaned = []\n",
    "    lines = output_str.splitlines()\n",
    "    in_fence = False\n",
    "    for line in lines:\n",
    "        if line.strip().startswith('```'):\n",
    "            in_fence = not in_fence\n",
    "            continue\n",
    "        if not in_fence:\n",
    "            cleaned.append(line)\n",
    "    text = '\\n'.join(cleaned)\n",
    "\n",
    "    # Save the cleaned text for debugging\n",
    "    with open('output/debug/task2_text.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # Split the text by double newlines to get each project idea block\n",
    "    idea_blocks = [block.strip() for block in text.split('\\n\\n') if block.strip()]\n",
    "\n",
    "    ideas = []\n",
    "    for idx, block in enumerate(idea_blocks, start=1):\n",
    "        # Initialize idea dictionary\n",
    "        idea = {'title': '', 'description': '', 'data_needs': []}\n",
    "\n",
    "        # Split the block into lines\n",
    "        block_lines = block.splitlines()\n",
    "\n",
    "        # Track which section we're currently processing\n",
    "        current_section = None\n",
    "\n",
    "        for line in block_lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Detect section headers\n",
    "            if line.startswith('TITLE:'):\n",
    "                current_section = 'title'\n",
    "                idea['title'] = line[6:].strip()  # Remove 'TITLE:' and whitespace\n",
    "            elif line.startswith('DESCRIPTION:'):\n",
    "                current_section = 'description'\n",
    "                idea['description'] = line[12:].strip()  # Remove 'DESCRIPTION:' and whitespace\n",
    "            elif line.startswith('DATA NEEDS:'):\n",
    "                current_section = 'data_needs'\n",
    "                # Split by commas and strip whitespace\n",
    "                data_items = [item.strip() for item in line[11:].split(',')]\n",
    "                idea['data_needs'] = [item for item in data_items if item]  # Filter out empty items\n",
    "            elif current_section:\n",
    "                # Continue adding content to the current section\n",
    "                if current_section == 'title':\n",
    "                    idea['title'] += ' ' + line\n",
    "                elif current_section == 'description':\n",
    "                    idea['description'] += ' ' + line\n",
    "                elif current_section == 'data_needs':\n",
    "                    # Handle multi-line data needs or add to existing data needs\n",
    "                    data_items = [item.strip() for item in line.split(',')]\n",
    "                    idea['data_needs'].extend([item for item in data_items if item])\n",
    "\n",
    "        # Create a safe ID from the title\n",
    "        safe_title = idea.get('title', '').lower().replace(' ', '_')[:50]\n",
    "        idea['idea_id'] = f\"{idx}_{safe_title}\"\n",
    "\n",
    "        ideas.append(idea)\n",
    "\n",
    "    return ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:30:02.389553Z",
     "start_time": "2025-05-09T18:29:47.679483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ideas...\n",
      "Parsing ideas...\n",
      "Found 20 ideas\n",
      "First idea example:\n",
      "Title: Arctic Anomaly Detection with Computer Vision\n",
      "Description: A deep learning system that detects and classifies Arctic climate anomalies (e.g., ice melt, temperature spikes) using satellite imagery and computer vision techniques. Leverages CNNs to identify patterns in multi-spectral data for real-time anomaly monitoring.\n",
      "Data needs: ['Satellite imagery', 'Arctic climate data', 'historical ice coverage records']\n",
      "Ideas saved to output/agent2_ideas.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    keyword_lists = load_keywords(KEYWORDS).get('keywords')\n",
    "    unique_papers = load_keywords(UNIQUE_PAPER)\n",
    "    all_ideas = []\n",
    "\n",
    "    print(\"Generating ideas...\")\n",
    "    llm_output = generate_ideas(keyword_lists, unique_papers, AGENT_2)\n",
    "\n",
    "    # Write raw output for debugging\n",
    "    with open('output/agent2_raw_llm_output.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(llm_output)\n",
    "\n",
    "    print(\"Parsing ideas...\")\n",
    "    ideas = parse_ideas(llm_output)\n",
    "    all_ideas.extend([i for i in ideas if i['title']])\n",
    "\n",
    "    print(f\"Found {len(all_ideas)} ideas\")\n",
    "\n",
    "    # Display first few ideas for verification\n",
    "    if all_ideas:\n",
    "        print(\"First idea example:\")\n",
    "        print(f\"Title: {all_ideas[0]['title']}\")\n",
    "        print(f\"Description: {all_ideas[0]['description']}\")\n",
    "        print(f\"Data needs: {all_ideas[0]['data_needs']}\")\n",
    "\n",
    "    # Write results to JSON\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_ideas, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Ideas saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T18:30:02.398862Z",
     "start_time": "2025-05-09T18:30:02.397602Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
